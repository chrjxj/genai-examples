{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbb9cfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2023, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a2b6260",
   "metadata": {},
   "source": [
    "# Use NVIDIA TensorRT to Accelerate Text to Video Inference\n",
    "\n",
    "\n",
    "**Author**: NVIDIA China SA team\n",
    "**Disclaim**: This tutorial only demonstrates TesorRT usage, and doesn't meant to provide any absolute performance value. Please test on your own enviorment. \n",
    "\n",
    "Text to Video is a popluar application since 2023, and research and industry communities have released many Text to Video models. \n",
    "\n",
    "In this tutorial, we use Alibaba DAMO's [text-to-video-synthesis](https://huggingface.co/ali-vilab/modelscope-damo-text-to-video-synthesis) model to demonstrate our recommended workflow to accelerate Text to Video Inference using NVIDIA TensorRT, and performance gain. \n",
    "\n",
    "We observed **2.2X inference speedup** on TensorRT compared native PyTorch, using the same GPU.\n",
    "\n",
    "Additional notes: \n",
    "\n",
    "* Some code came from  https://github.com/modelscope/modelscope/tree/v1.11.1/modelscope/models/multi_modal/video_synthesis, with small modifications\n",
    "\n",
    "\n",
    "## Before Start\n",
    "\n",
    "Before start, please prepare \n",
    "\n",
    "- 1 * NVIDIA GPU, at least 48GB GPU RAM; Ampere, Hopper or Ada \n",
    "- Driver Version: 535.104.12   CUDA Version: 12.2\n",
    "- Docker and [nvidia-container-toolkit](https://github.com/NVIDIA/nvidia-container-toolkit)\n",
    "- Access to NVIDIA NGC https://ngc.nvidia.com/ (we will use docker image from NGC)\n",
    "\n",
    "## Build Docker image\n",
    "\n",
    "Before using this notebook, you need to build docker image and start a container\n",
    "\n",
    "build docker image:\n",
    "\n",
    "```bash\n",
    "cd docker\n",
    "docker build -t nvcr.io/nvidia/tensorrt:22.11-py3-txt2video .\n",
    "```\n",
    "\n",
    "\n",
    "start a container:\n",
    "\n",
    "```bash\n",
    "docker run --gpus '\"device=0\"' -it --shm-size=8G --rm --net=host --ipc=host \\\n",
    "        --ulimit memlock=-1 --ulimit stack=67108864 \\\n",
    "        -v `pwd`:/workspace \\\n",
    "        nvcr.io/nvidia/tensorrt:22.11-py3-txt2video bash\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11adb24",
   "metadata": {},
   "source": [
    "## Download Model Files from HuggingFace\n",
    "\n",
    "You need to download model files from https://huggingface.co/ali-vilab/modelscope-damo-text-to-video-synthesis/tree/main\n",
    "\n",
    "You can copy your HF token from https://huggingface.co website, and use following script to download models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007dd1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "repo = \"ali-vilab/modelscope-damo-text-to-video-synthesis\"\n",
    "output_dir = \"./models\"\n",
    "HF_TOKEN = <<YOUR_HF_TOKEN>>\n",
    "snapshot_download(repo_id=repo, local_dir=output_dir, local_dir_use_symlinks=False, token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453465f1",
   "metadata": {},
   "source": [
    "## Build TensorRT engine\n",
    "\n",
    "- **Step 1** Convert models from pytorch to Onnx\n",
    "- **Step 2** Build TensorRT engine from onnx files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d849c5c",
   "metadata": {},
   "source": [
    "#### Step 1 - Convert models from pytorch to Onnx\n",
    "\n",
    "we use `torch.onnx.export` to convert model from pytorch to onnx format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2edd0d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os import path as osp\n",
    "from unet_sd import UNetSD\n",
    "import torch\n",
    "\n",
    "def convert_to_onnx(model_dir, onnx_dir, device=0):\n",
    "\n",
    "    config = json.load(open(osp.join(model_dir, \"configuration.json\")))\n",
    "    cfg = config[\"model\"][\"model_cfg\"]\n",
    "    cfg['temporal_attention'] = True if cfg[\n",
    "        'temporal_attention'] == 'True' else False\n",
    "\n",
    "    # Initialize unet\n",
    "    sd_model = UNetSD(\n",
    "        in_dim=cfg['unet_in_dim'],\n",
    "        dim=cfg['unet_dim'],\n",
    "        y_dim=cfg['unet_y_dim'],\n",
    "        context_dim=cfg['unet_context_dim'],\n",
    "        out_dim=cfg['unet_out_dim'],\n",
    "        dim_mult=cfg['unet_dim_mult'],\n",
    "        num_heads=cfg['unet_num_heads'],\n",
    "        head_dim=cfg['unet_head_dim'],\n",
    "        num_res_blocks=cfg['unet_res_blocks'],\n",
    "        attn_scales=cfg['unet_attn_scales'],\n",
    "        dropout=cfg['unet_dropout'],\n",
    "        temporal_attention=cfg['temporal_attention'])\n",
    "    sd_model.load_state_dict(\n",
    "        torch.load(\n",
    "            osp.join(model_dir, config[\"model\"][\"model_args\"][\"ckpt_unet\"])),\n",
    "        strict=True)\n",
    "    sd_model.eval()\n",
    "    sd_model.to(device)\n",
    "\n",
    "\n",
    "    dummy_x = torch.randn(1, 4, config[\"model\"][\"model_args\"][\"max_frames\"],\n",
    "         config[\"model\"][\"model_args\"].get(\"width\", 256)//8,\n",
    "         config[\"model\"][\"model_args\"].get(\"width\", 256)//8,\n",
    "         dtype=torch.float32, device=0).cuda()  # noise tensor\n",
    "    dummy_t = torch.randint(0, 1000, (1,), dtype=torch.int64, device=0).cuda()  # time steps tensor\n",
    "    dummy_y = torch.randn(1, 77, 1024, dtype=torch.float32, device=0).cuda()  # text embeddings tensor\n",
    "\n",
    "    # Export the model to ONNX, onnx->trt will failed if add dynamic axes\n",
    "    onnx_file_path = osp.join(onnx_dir, \"model.onnx\")\n",
    "    torch.onnx.export(sd_model,\n",
    "                    (dummy_x, dummy_t, dummy_y),\n",
    "                    onnx_file_path,\n",
    "                    export_params=True,\n",
    "                    opset_version=17,\n",
    "                    do_constant_folding=True,\n",
    "                    input_names=['x', 't', 'y'],\n",
    "                    output_names=['output'],\n",
    "                    )\n",
    "\n",
    "    print(\"Convert to ONNX finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41a615b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/text_to_video/unet_sd.py:280: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if mask_last_frame_num > 0:\n",
      "/workspace/text_to_video/unet_sd.py:1073: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if prob == 1:\n",
      "/workspace/text_to_video/unet_sd.py:1075: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  elif prob == 0:\n",
      "/workspace/text_to_video/unet_sd.py:300: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  f = torch.tensor(f, dtype=torch.int32).to(0)\n",
      "/workspace/text_to_video/unet_sd.py:300: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  f = torch.tensor(f, dtype=torch.int32).to(0)\n",
      "/workspace/text_to_video/unet_sd.py:901: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert x.shape[1] == self.channels\n",
      "/workspace/text_to_video/unet_sd.py:730: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert x.shape[1] == self.channels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert to ONNX finished\n"
     ]
    }
   ],
   "source": [
    "# Convert to ONNX\n",
    "model_dir = \"./models\"\n",
    "onnx_dir = \"./models/onnx_test\"\n",
    "convert_to_onnx(model_dir, onnx_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eb196a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 5.3G\r\n",
      "-rw-r--r-- 1 root root  45K Feb  4 16:03 input_blocks.0.0.weight\r\n",
      "-rw-r--r-- 1 root root 640K Feb  4 16:03 input_blocks.0.1.proj_in.weight\r\n",
      "-rw-r--r-- 1 root root 640K Feb  4 16:03 input_blocks.0.1.proj_out.weight\r\n",
      "-rw-r--r-- 1 root root  16K Feb  4 16:03 input_blocks.0.1.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 input_blocks.1.0.emb_layers.1.weight\r\n",
      "-rw-r--r-- 1 root root 3.6M Feb  4 16:03 input_blocks.1.0.in_layers.2.weight\r\n",
      "-rw-r--r-- 1 root root 3.6M Feb  4 16:03 input_blocks.1.0.out_layers.3.weight\r\n",
      "-rw-r--r-- 1 root root 1.2M Feb  4 16:03 input_blocks.1.0.temopral_conv.conv1.2.weight\r\n",
      "-rw-r--r-- 1 root root 1.2M Feb  4 16:03 input_blocks.1.0.temopral_conv.conv2.3.weight\r\n",
      "-rw-r--r-- 1 root root 1.2M Feb  4 16:03 input_blocks.1.0.temopral_conv.conv3.3.weight\r\n",
      "-rw-r--r-- 1 root root 1.2M Feb  4 16:03 input_blocks.1.0.temopral_conv.conv4.3.weight\r\n",
      "-rw-r--r-- 1 root root  10K Feb  4 16:03 input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 input_blocks.1.2.proj_in.weight\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 input_blocks.1.2.proj_out.weight\r\n",
      "-rw-r--r-- 1 root root  10K Feb  4 16:03 input_blocks.1.2.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.10.0.emb_layers.1.bias\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 input_blocks.10.0.emb_layers.1.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.10.0.in_layers.2.bias\r\n",
      "-rw-r--r-- 1 root root  57M Feb  4 16:03 input_blocks.10.0.in_layers.2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.10.0.out_layers.3.bias\r\n",
      "-rw-r--r-- 1 root root  57M Feb  4 16:03 input_blocks.10.0.out_layers.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.10.0.temopral_conv.conv1.2.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 input_blocks.10.0.temopral_conv.conv1.2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.10.0.temopral_conv.conv2.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 input_blocks.10.0.temopral_conv.conv2.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.10.0.temopral_conv.conv3.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 input_blocks.10.0.temopral_conv.conv3.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.10.0.temopral_conv.conv4.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 input_blocks.10.0.temopral_conv.conv4.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.11.0.emb_layers.1.bias\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 input_blocks.11.0.emb_layers.1.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.11.0.in_layers.2.bias\r\n",
      "-rw-r--r-- 1 root root  57M Feb  4 16:03 input_blocks.11.0.in_layers.2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.11.0.out_layers.3.bias\r\n",
      "-rw-r--r-- 1 root root  57M Feb  4 16:03 input_blocks.11.0.out_layers.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.11.0.temopral_conv.conv1.2.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 input_blocks.11.0.temopral_conv.conv1.2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.11.0.temopral_conv.conv2.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 input_blocks.11.0.temopral_conv.conv2.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.11.0.temopral_conv.conv3.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 input_blocks.11.0.temopral_conv.conv3.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.11.0.temopral_conv.conv4.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 input_blocks.11.0.temopral_conv.conv4.3.weight\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 input_blocks.2.0.emb_layers.1.weight\r\n",
      "-rw-r--r-- 1 root root 3.6M Feb  4 16:03 input_blocks.2.0.in_layers.2.weight\r\n",
      "-rw-r--r-- 1 root root 3.6M Feb  4 16:03 input_blocks.2.0.out_layers.3.weight\r\n",
      "-rw-r--r-- 1 root root 1.2M Feb  4 16:03 input_blocks.2.0.temopral_conv.conv1.2.weight\r\n",
      "-rw-r--r-- 1 root root 1.2M Feb  4 16:03 input_blocks.2.0.temopral_conv.conv2.3.weight\r\n",
      "-rw-r--r-- 1 root root 1.2M Feb  4 16:03 input_blocks.2.0.temopral_conv.conv3.3.weight\r\n",
      "-rw-r--r-- 1 root root 1.2M Feb  4 16:03 input_blocks.2.0.temopral_conv.conv4.3.weight\r\n",
      "-rw-r--r-- 1 root root  10K Feb  4 16:03 input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 input_blocks.2.2.proj_in.weight\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 input_blocks.2.2.proj_out.weight\r\n",
      "-rw-r--r-- 1 root root  10K Feb  4 16:03 input_blocks.2.2.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 3.6M Feb  4 16:03 input_blocks.3.op.weight\r\n",
      "-rw-r--r-- 1 root root 3.2M Feb  4 16:03 input_blocks.4.0.emb_layers.1.weight\r\n",
      "-rw-r--r-- 1 root root 7.1M Feb  4 16:03 input_blocks.4.0.in_layers.2.weight\r\n",
      "-rw-r--r-- 1 root root  15M Feb  4 16:03 input_blocks.4.0.out_layers.3.weight\r\n",
      "-rw-r--r-- 1 root root 800K Feb  4 16:03 input_blocks.4.0.skip_connection.weight\r\n",
      "-rw-r--r-- 1 root root 4.7M Feb  4 16:03 input_blocks.4.0.temopral_conv.conv1.2.weight\r\n",
      "-rw-r--r-- 1 root root 4.7M Feb  4 16:03 input_blocks.4.0.temopral_conv.conv2.3.weight\r\n",
      "-rw-r--r-- 1 root root 4.7M Feb  4 16:03 input_blocks.4.0.temopral_conv.conv3.3.weight\r\n",
      "-rw-r--r-- 1 root root 4.7M Feb  4 16:03 input_blocks.4.0.temopral_conv.conv4.3.weight\r\n",
      "-rw-r--r-- 1 root root  20K Feb  4 16:03 input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 input_blocks.4.2.proj_in.weight\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 input_blocks.4.2.proj_out.weight\r\n",
      "-rw-r--r-- 1 root root  20K Feb  4 16:03 input_blocks.4.2.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 3.2M Feb  4 16:03 input_blocks.5.0.emb_layers.1.weight\r\n",
      "-rw-r--r-- 1 root root  15M Feb  4 16:03 input_blocks.5.0.in_layers.2.weight\r\n",
      "-rw-r--r-- 1 root root  15M Feb  4 16:03 input_blocks.5.0.out_layers.3.weight\r\n",
      "-rw-r--r-- 1 root root 4.7M Feb  4 16:03 input_blocks.5.0.temopral_conv.conv1.2.weight\r\n",
      "-rw-r--r-- 1 root root 4.7M Feb  4 16:03 input_blocks.5.0.temopral_conv.conv2.3.weight\r\n",
      "-rw-r--r-- 1 root root 4.7M Feb  4 16:03 input_blocks.5.0.temopral_conv.conv3.3.weight\r\n",
      "-rw-r--r-- 1 root root 4.7M Feb  4 16:03 input_blocks.5.0.temopral_conv.conv4.3.weight\r\n",
      "-rw-r--r-- 1 root root  20K Feb  4 16:03 input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 input_blocks.5.2.proj_in.weight\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 input_blocks.5.2.proj_out.weight\r\n",
      "-rw-r--r-- 1 root root  20K Feb  4 16:03 input_blocks.5.2.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root  15M Feb  4 16:03 input_blocks.6.op.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.0.emb_layers.1.bias\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 input_blocks.7.0.emb_layers.1.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.0.in_layers.2.bias\r\n",
      "-rw-r--r-- 1 root root  29M Feb  4 16:03 input_blocks.7.0.in_layers.2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.0.out_layers.3.bias\r\n",
      "-rw-r--r-- 1 root root  57M Feb  4 16:03 input_blocks.7.0.out_layers.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.0.skip_connection.bias\r\n",
      "-rw-r--r-- 1 root root 3.2M Feb  4 16:03 input_blocks.7.0.skip_connection.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.0.temopral_conv.conv1.2.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 input_blocks.7.0.temopral_conv.conv1.2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.0.temopral_conv.conv2.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 input_blocks.7.0.temopral_conv.conv2.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.0.temopral_conv.conv3.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 input_blocks.7.0.temopral_conv.conv3.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.0.temopral_conv.conv4.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 input_blocks.7.0.temopral_conv.conv4.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.1.proj_in.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.1.proj_out.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias\r\n",
      "-rw-r--r-- 1 root root  40K Feb  4 16:03 input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.1.transformer_blocks.0.ff.net.2.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.1.transformer_blocks.0.norm1.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.1.transformer_blocks.0.norm1.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.1.transformer_blocks.0.norm2.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.1.transformer_blocks.0.norm2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.1.transformer_blocks.0.norm3.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.1.transformer_blocks.0.norm3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.2.proj_in.bias\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 input_blocks.7.2.proj_in.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.2.proj_out.bias\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 input_blocks.7.2.proj_out.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.2.transformer_blocks.0.attn1.to_out.0.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.2.transformer_blocks.0.attn2.to_out.0.bias\r\n",
      "-rw-r--r-- 1 root root  40K Feb  4 16:03 input_blocks.7.2.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.2.transformer_blocks.0.ff.net.2.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.2.transformer_blocks.0.norm1.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.2.transformer_blocks.0.norm1.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.2.transformer_blocks.0.norm2.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.2.transformer_blocks.0.norm2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.2.transformer_blocks.0.norm3.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.7.2.transformer_blocks.0.norm3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.0.emb_layers.1.bias\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 input_blocks.8.0.emb_layers.1.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.0.in_layers.2.bias\r\n",
      "-rw-r--r-- 1 root root  57M Feb  4 16:03 input_blocks.8.0.in_layers.2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.0.out_layers.3.bias\r\n",
      "-rw-r--r-- 1 root root  57M Feb  4 16:03 input_blocks.8.0.out_layers.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.0.temopral_conv.conv1.2.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 input_blocks.8.0.temopral_conv.conv1.2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.0.temopral_conv.conv2.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 input_blocks.8.0.temopral_conv.conv2.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.0.temopral_conv.conv3.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 input_blocks.8.0.temopral_conv.conv3.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.0.temopral_conv.conv4.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 input_blocks.8.0.temopral_conv.conv4.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.1.proj_in.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.1.proj_out.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias\r\n",
      "-rw-r--r-- 1 root root  40K Feb  4 16:03 input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.1.transformer_blocks.0.ff.net.2.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.1.transformer_blocks.0.norm1.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.1.transformer_blocks.0.norm1.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.1.transformer_blocks.0.norm2.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.1.transformer_blocks.0.norm2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.1.transformer_blocks.0.norm3.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.1.transformer_blocks.0.norm3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.2.proj_in.bias\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 input_blocks.8.2.proj_in.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.2.proj_out.bias\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 input_blocks.8.2.proj_out.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.2.transformer_blocks.0.attn1.to_out.0.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.2.transformer_blocks.0.attn2.to_out.0.bias\r\n",
      "-rw-r--r-- 1 root root  40K Feb  4 16:03 input_blocks.8.2.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.2.transformer_blocks.0.ff.net.2.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.2.transformer_blocks.0.norm1.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.2.transformer_blocks.0.norm1.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.2.transformer_blocks.0.norm2.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.2.transformer_blocks.0.norm2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.2.transformer_blocks.0.norm3.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.8.2.transformer_blocks.0.norm3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 input_blocks.9.op.bias\r\n",
      "-rw-r--r-- 1 root root  57M Feb  4 16:03 input_blocks.9.op.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.0.emb_layers.1.bias\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 middle_block.0.emb_layers.1.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.0.in_layers.2.bias\r\n",
      "-rw-r--r-- 1 root root  57M Feb  4 16:03 middle_block.0.in_layers.2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.0.out_layers.3.bias\r\n",
      "-rw-r--r-- 1 root root  57M Feb  4 16:03 middle_block.0.out_layers.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.0.temopral_conv.conv1.2.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 middle_block.0.temopral_conv.conv1.2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.0.temopral_conv.conv2.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 middle_block.0.temopral_conv.conv2.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.0.temopral_conv.conv3.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 middle_block.0.temopral_conv.conv3.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.0.temopral_conv.conv4.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 middle_block.0.temopral_conv.conv4.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.1.proj_in.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.1.proj_out.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.1.transformer_blocks.0.attn1.to_out.0.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.1.transformer_blocks.0.attn2.to_out.0.bias\r\n",
      "-rw-r--r-- 1 root root  40K Feb  4 16:03 middle_block.1.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.1.transformer_blocks.0.ff.net.2.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.1.transformer_blocks.0.norm1.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.1.transformer_blocks.0.norm1.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.1.transformer_blocks.0.norm2.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.1.transformer_blocks.0.norm2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.1.transformer_blocks.0.norm3.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.1.transformer_blocks.0.norm3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.2.proj_in.bias\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 middle_block.2.proj_in.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.2.proj_out.bias\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 middle_block.2.proj_out.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.2.transformer_blocks.0.attn1.to_out.0.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.2.transformer_blocks.0.attn2.to_out.0.bias\r\n",
      "-rw-r--r-- 1 root root  40K Feb  4 16:03 middle_block.2.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.2.transformer_blocks.0.ff.net.2.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.2.transformer_blocks.0.norm1.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.2.transformer_blocks.0.norm1.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.2.transformer_blocks.0.norm2.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.2.transformer_blocks.0.norm2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.2.transformer_blocks.0.norm3.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.2.transformer_blocks.0.norm3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.3.emb_layers.1.bias\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 middle_block.3.emb_layers.1.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.3.in_layers.2.bias\r\n",
      "-rw-r--r-- 1 root root  57M Feb  4 16:03 middle_block.3.in_layers.2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.3.out_layers.3.bias\r\n",
      "-rw-r--r-- 1 root root  57M Feb  4 16:03 middle_block.3.out_layers.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.3.temopral_conv.conv1.2.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 middle_block.3.temopral_conv.conv1.2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.3.temopral_conv.conv2.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 middle_block.3.temopral_conv.conv2.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.3.temopral_conv.conv3.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 middle_block.3.temopral_conv.conv3.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 middle_block.3.temopral_conv.conv4.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 middle_block.3.temopral_conv.conv4.3.weight\r\n",
      "-rw-r--r-- 1 root root 4.3M Feb  4 16:03 model.onnx\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20615\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20618\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20620\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20622\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20624\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20626\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20665\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20685\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20687\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20690\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20692\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20694\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20696\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20698\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20737\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20757\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20759\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20762\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20764\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20766\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20768\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20770\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20772\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20775\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20777\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20779\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20781\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20783\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20785\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20788\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20790\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20792\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20794\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20796\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20835\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20855\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20857\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20860\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20862\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20864\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20866\r\n",
      "-rw-r--r-- 1 root root  10K Feb  4 16:03 onnx__Add_20868\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20870\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20873\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20875\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20877\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20879\r\n",
      "-rw-r--r-- 1 root root  10K Feb  4 16:03 onnx__Add_20881\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20883\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20886\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20888\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20890\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20892\r\n",
      "-rw-r--r-- 1 root root  10K Feb  4 16:03 onnx__Add_20894\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20896\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20899\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20901\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20903\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20905\r\n",
      "-rw-r--r-- 1 root root  10K Feb  4 16:03 onnx__Add_20908\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20910\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20913\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20915\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20917\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20919\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20921\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20960\r\n",
      "-rw-r--r-- 1 root root  10K Feb  4 16:03 onnx__Add_20980\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20982\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20985\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20987\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20989\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20991\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_20993\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_21032\r\n",
      "-rw-r--r-- 1 root root 7.5K Feb  4 16:03 onnx__Add_21052\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_21054\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_21057\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_21059\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_21061\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_21063\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_21065\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_21104\r\n",
      "-rw-r--r-- 1 root root 7.5K Feb  4 16:03 onnx__Add_21125\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Add_21197\r\n",
      "-rw-r--r-- 1 root root 1.0M Feb  4 16:03 onnx__MatMul_20278\r\n",
      "-rw-r--r-- 1 root root 1.0M Feb  4 16:03 onnx__MatMul_20279\r\n",
      "-rw-r--r-- 1 root root 1.0M Feb  4 16:03 onnx__MatMul_20280\r\n",
      "-rw-r--r-- 1 root root 1.0M Feb  4 16:03 onnx__MatMul_20309\r\n",
      "-rw-r--r-- 1 root root 1.0M Feb  4 16:03 onnx__MatMul_20310\r\n",
      "-rw-r--r-- 1 root root 1.0M Feb  4 16:03 onnx__MatMul_20311\r\n",
      "-rw-r--r-- 1 root root 1.0M Feb  4 16:03 onnx__MatMul_20312\r\n",
      "-rw-r--r-- 1 root root 1.0M Feb  4 16:03 onnx__MatMul_20317\r\n",
      "-rw-r--r-- 1 root root 8.0M Feb  4 16:03 onnx__MatMul_20318\r\n",
      "-rw-r--r-- 1 root root 4.0M Feb  4 16:03 onnx__MatMul_20319\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20339\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20340\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20341\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20342\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20347\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20348\r\n",
      "-rw-r--r-- 1 root root 1.3M Feb  4 16:03 onnx__MatMul_20349\r\n",
      "-rw-r--r-- 1 root root 1.3M Feb  4 16:03 onnx__MatMul_20350\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20371\r\n",
      "-rw-r--r-- 1 root root 3.2M Feb  4 16:03 onnx__MatMul_20372\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20373\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20374\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20378\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20379\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20380\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20385\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20386\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20387\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20388\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20393\r\n",
      "-rw-r--r-- 1 root root 3.2M Feb  4 16:03 onnx__MatMul_20394\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20395\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20411\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20412\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20413\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20414\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20419\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20420\r\n",
      "-rw-r--r-- 1 root root 1.3M Feb  4 16:03 onnx__MatMul_20421\r\n",
      "-rw-r--r-- 1 root root 1.3M Feb  4 16:03 onnx__MatMul_20422\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20443\r\n",
      "-rw-r--r-- 1 root root 3.2M Feb  4 16:03 onnx__MatMul_20444\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20445\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20446\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20450\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20451\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20452\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20457\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20458\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20459\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20460\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_20465\r\n",
      "-rw-r--r-- 1 root root 3.2M Feb  4 16:03 onnx__MatMul_20466\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20467\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20483\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20484\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20485\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20486\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20491\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20492\r\n",
      "-rw-r--r-- 1 root root 2.5M Feb  4 16:03 onnx__MatMul_20493\r\n",
      "-rw-r--r-- 1 root root 2.5M Feb  4 16:03 onnx__MatMul_20494\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20515\r\n",
      "-rw-r--r-- 1 root root  13M Feb  4 16:03 onnx__MatMul_20516\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20517\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20518\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20522\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20523\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20524\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20529\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20530\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20531\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20532\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20537\r\n",
      "-rw-r--r-- 1 root root  13M Feb  4 16:03 onnx__MatMul_20538\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20539\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20555\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20556\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20557\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20558\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20563\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20564\r\n",
      "-rw-r--r-- 1 root root 2.5M Feb  4 16:03 onnx__MatMul_20565\r\n",
      "-rw-r--r-- 1 root root 2.5M Feb  4 16:03 onnx__MatMul_20566\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20587\r\n",
      "-rw-r--r-- 1 root root  13M Feb  4 16:03 onnx__MatMul_20588\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20589\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20590\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20594\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20595\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20596\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20601\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20602\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20603\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20604\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_20609\r\n",
      "-rw-r--r-- 1 root root  13M Feb  4 16:03 onnx__MatMul_20610\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20611\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20627\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20628\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20629\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20630\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20635\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20636\r\n",
      "-rw-r--r-- 1 root root 5.0M Feb  4 16:03 onnx__MatMul_20637\r\n",
      "-rw-r--r-- 1 root root 5.0M Feb  4 16:03 onnx__MatMul_20638\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20659\r\n",
      "-rw-r--r-- 1 root root  50M Feb  4 16:03 onnx__MatMul_20660\r\n",
      "-rw-r--r-- 1 root root  25M Feb  4 16:03 onnx__MatMul_20661\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20662\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20666\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20667\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20668\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20673\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20674\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20675\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20676\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20681\r\n",
      "-rw-r--r-- 1 root root  50M Feb  4 16:03 onnx__MatMul_20682\r\n",
      "-rw-r--r-- 1 root root  25M Feb  4 16:03 onnx__MatMul_20683\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20699\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20700\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20701\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20702\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20707\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20708\r\n",
      "-rw-r--r-- 1 root root 5.0M Feb  4 16:03 onnx__MatMul_20709\r\n",
      "-rw-r--r-- 1 root root 5.0M Feb  4 16:03 onnx__MatMul_20710\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20731\r\n",
      "-rw-r--r-- 1 root root  50M Feb  4 16:03 onnx__MatMul_20732\r\n",
      "-rw-r--r-- 1 root root  25M Feb  4 16:03 onnx__MatMul_20733\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20734\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20738\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20739\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20740\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20745\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20746\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20747\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20748\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20753\r\n",
      "-rw-r--r-- 1 root root  50M Feb  4 16:03 onnx__MatMul_20754\r\n",
      "-rw-r--r-- 1 root root  25M Feb  4 16:03 onnx__MatMul_20755\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20797\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20798\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20799\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20800\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20805\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20806\r\n",
      "-rw-r--r-- 1 root root 5.0M Feb  4 16:03 onnx__MatMul_20807\r\n",
      "-rw-r--r-- 1 root root 5.0M Feb  4 16:03 onnx__MatMul_20808\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20829\r\n",
      "-rw-r--r-- 1 root root  50M Feb  4 16:03 onnx__MatMul_20830\r\n",
      "-rw-r--r-- 1 root root  25M Feb  4 16:03 onnx__MatMul_20831\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20832\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20836\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20837\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20838\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20843\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20844\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20845\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20846\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20851\r\n",
      "-rw-r--r-- 1 root root  50M Feb  4 16:03 onnx__MatMul_20852\r\n",
      "-rw-r--r-- 1 root root  25M Feb  4 16:03 onnx__MatMul_20853\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20922\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20923\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20924\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20925\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20930\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20931\r\n",
      "-rw-r--r-- 1 root root 5.0M Feb  4 16:03 onnx__MatMul_20932\r\n",
      "-rw-r--r-- 1 root root 5.0M Feb  4 16:03 onnx__MatMul_20933\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20954\r\n",
      "-rw-r--r-- 1 root root  50M Feb  4 16:03 onnx__MatMul_20955\r\n",
      "-rw-r--r-- 1 root root  25M Feb  4 16:03 onnx__MatMul_20956\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20957\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20961\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20962\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20963\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20968\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20969\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20970\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20971\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20976\r\n",
      "-rw-r--r-- 1 root root  50M Feb  4 16:03 onnx__MatMul_20977\r\n",
      "-rw-r--r-- 1 root root  25M Feb  4 16:03 onnx__MatMul_20978\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20994\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20995\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20996\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_20997\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21002\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21003\r\n",
      "-rw-r--r-- 1 root root 5.0M Feb  4 16:03 onnx__MatMul_21004\r\n",
      "-rw-r--r-- 1 root root 5.0M Feb  4 16:03 onnx__MatMul_21005\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21026\r\n",
      "-rw-r--r-- 1 root root  50M Feb  4 16:03 onnx__MatMul_21027\r\n",
      "-rw-r--r-- 1 root root  25M Feb  4 16:03 onnx__MatMul_21028\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21029\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21033\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21034\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21035\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21040\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21041\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21042\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21043\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21048\r\n",
      "-rw-r--r-- 1 root root  50M Feb  4 16:03 onnx__MatMul_21049\r\n",
      "-rw-r--r-- 1 root root  25M Feb  4 16:03 onnx__MatMul_21050\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21066\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21067\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21068\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21069\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21074\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21075\r\n",
      "-rw-r--r-- 1 root root 5.0M Feb  4 16:03 onnx__MatMul_21076\r\n",
      "-rw-r--r-- 1 root root 5.0M Feb  4 16:03 onnx__MatMul_21077\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21098\r\n",
      "-rw-r--r-- 1 root root  50M Feb  4 16:03 onnx__MatMul_21099\r\n",
      "-rw-r--r-- 1 root root  25M Feb  4 16:03 onnx__MatMul_21100\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21101\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21105\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21106\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21107\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21112\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21113\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21114\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21115\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21120\r\n",
      "-rw-r--r-- 1 root root  50M Feb  4 16:03 onnx__MatMul_21121\r\n",
      "-rw-r--r-- 1 root root  25M Feb  4 16:03 onnx__MatMul_21122\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21139\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21140\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21141\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21142\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21147\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21148\r\n",
      "-rw-r--r-- 1 root root 2.5M Feb  4 16:03 onnx__MatMul_21149\r\n",
      "-rw-r--r-- 1 root root 2.5M Feb  4 16:03 onnx__MatMul_21150\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21171\r\n",
      "-rw-r--r-- 1 root root  13M Feb  4 16:03 onnx__MatMul_21172\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21173\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21174\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21178\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21179\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21180\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21185\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21186\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21187\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21188\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21193\r\n",
      "-rw-r--r-- 1 root root  13M Feb  4 16:03 onnx__MatMul_21194\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21195\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21211\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21212\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21213\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21214\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21219\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21220\r\n",
      "-rw-r--r-- 1 root root 2.5M Feb  4 16:03 onnx__MatMul_21221\r\n",
      "-rw-r--r-- 1 root root 2.5M Feb  4 16:03 onnx__MatMul_21222\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21243\r\n",
      "-rw-r--r-- 1 root root  13M Feb  4 16:03 onnx__MatMul_21244\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21245\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21246\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21250\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21251\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21252\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21257\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21258\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21259\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21260\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21265\r\n",
      "-rw-r--r-- 1 root root  13M Feb  4 16:03 onnx__MatMul_21266\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21267\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21283\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21284\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21285\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21286\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21291\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21292\r\n",
      "-rw-r--r-- 1 root root 2.5M Feb  4 16:03 onnx__MatMul_21293\r\n",
      "-rw-r--r-- 1 root root 2.5M Feb  4 16:03 onnx__MatMul_21294\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21315\r\n",
      "-rw-r--r-- 1 root root  13M Feb  4 16:03 onnx__MatMul_21316\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21317\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21318\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21322\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21323\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21324\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21329\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21330\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21331\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21332\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21337\r\n",
      "-rw-r--r-- 1 root root  13M Feb  4 16:03 onnx__MatMul_21338\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 onnx__MatMul_21339\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21356\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21357\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21358\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21359\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21364\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21365\r\n",
      "-rw-r--r-- 1 root root 1.3M Feb  4 16:03 onnx__MatMul_21366\r\n",
      "-rw-r--r-- 1 root root 1.3M Feb  4 16:03 onnx__MatMul_21367\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21388\r\n",
      "-rw-r--r-- 1 root root 3.2M Feb  4 16:03 onnx__MatMul_21389\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21390\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21391\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21395\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21396\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21397\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21402\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21403\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21404\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21405\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21410\r\n",
      "-rw-r--r-- 1 root root 3.2M Feb  4 16:03 onnx__MatMul_21411\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21412\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21428\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21429\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21430\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21431\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21436\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21437\r\n",
      "-rw-r--r-- 1 root root 1.3M Feb  4 16:03 onnx__MatMul_21438\r\n",
      "-rw-r--r-- 1 root root 1.3M Feb  4 16:03 onnx__MatMul_21439\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21460\r\n",
      "-rw-r--r-- 1 root root 3.2M Feb  4 16:03 onnx__MatMul_21461\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21462\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21463\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21467\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21468\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21469\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21474\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21475\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21476\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21477\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21482\r\n",
      "-rw-r--r-- 1 root root 3.2M Feb  4 16:03 onnx__MatMul_21483\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21484\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21500\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21501\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21502\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21503\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21508\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21509\r\n",
      "-rw-r--r-- 1 root root 1.3M Feb  4 16:03 onnx__MatMul_21510\r\n",
      "-rw-r--r-- 1 root root 1.3M Feb  4 16:03 onnx__MatMul_21511\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21532\r\n",
      "-rw-r--r-- 1 root root 3.2M Feb  4 16:03 onnx__MatMul_21533\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21534\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21535\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21539\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21540\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21541\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21546\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21547\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21548\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21549\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 onnx__MatMul_21554\r\n",
      "-rw-r--r-- 1 root root 3.2M Feb  4 16:03 onnx__MatMul_21555\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 onnx__MatMul_21556\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20614\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20617\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20619\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20621\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20623\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20625\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20664\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20684\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20686\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20689\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20691\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20693\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20695\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20697\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20736\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20756\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20758\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20761\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20763\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20765\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20767\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20769\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20771\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20774\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20776\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20778\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20780\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20782\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20784\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20787\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20789\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20791\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20793\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20795\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20834\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20854\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20856\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20859\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20861\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20863\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20865\r\n",
      "-rw-r--r-- 1 root root  10K Feb  4 16:03 onnx__Mul_20867\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20869\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20872\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20874\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20876\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20878\r\n",
      "-rw-r--r-- 1 root root  10K Feb  4 16:03 onnx__Mul_20880\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20882\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20885\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20887\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20889\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20891\r\n",
      "-rw-r--r-- 1 root root  10K Feb  4 16:03 onnx__Mul_20893\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20895\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20898\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20900\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20902\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20904\r\n",
      "-rw-r--r-- 1 root root  10K Feb  4 16:03 onnx__Mul_20907\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20909\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20912\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20914\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20916\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20918\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20920\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20959\r\n",
      "-rw-r--r-- 1 root root  10K Feb  4 16:03 onnx__Mul_20979\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20981\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20984\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20986\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20988\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20990\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_20992\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_21031\r\n",
      "-rw-r--r-- 1 root root 7.5K Feb  4 16:03 onnx__Mul_21051\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_21053\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_21056\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_21058\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_21060\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_21062\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_21064\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_21103\r\n",
      "-rw-r--r-- 1 root root 7.5K Feb  4 16:03 onnx__Mul_21124\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 onnx__Mul_21196\r\n",
      "-rw-r--r-- 1 root root  45K Feb  4 16:03 out.2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.0.0.emb_layers.1.bias\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 output_blocks.0.0.emb_layers.1.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.0.0.in_layers.2.bias\r\n",
      "-rw-r--r-- 1 root root 113M Feb  4 16:03 output_blocks.0.0.in_layers.2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.0.0.out_layers.3.bias\r\n",
      "-rw-r--r-- 1 root root  57M Feb  4 16:03 output_blocks.0.0.out_layers.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.0.0.skip_connection.bias\r\n",
      "-rw-r--r-- 1 root root  13M Feb  4 16:03 output_blocks.0.0.skip_connection.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.0.0.temopral_conv.conv1.2.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 output_blocks.0.0.temopral_conv.conv1.2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.0.0.temopral_conv.conv2.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 output_blocks.0.0.temopral_conv.conv2.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.0.0.temopral_conv.conv3.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 output_blocks.0.0.temopral_conv.conv3.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.0.0.temopral_conv.conv4.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 output_blocks.0.0.temopral_conv.conv4.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.1.0.emb_layers.1.bias\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 output_blocks.1.0.emb_layers.1.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.1.0.in_layers.2.bias\r\n",
      "-rw-r--r-- 1 root root 113M Feb  4 16:03 output_blocks.1.0.in_layers.2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.1.0.out_layers.3.bias\r\n",
      "-rw-r--r-- 1 root root  57M Feb  4 16:03 output_blocks.1.0.out_layers.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.1.0.skip_connection.bias\r\n",
      "-rw-r--r-- 1 root root  13M Feb  4 16:03 output_blocks.1.0.skip_connection.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.1.0.temopral_conv.conv1.2.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 output_blocks.1.0.temopral_conv.conv1.2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.1.0.temopral_conv.conv2.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 output_blocks.1.0.temopral_conv.conv2.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.1.0.temopral_conv.conv3.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 output_blocks.1.0.temopral_conv.conv3.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.1.0.temopral_conv.conv4.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 output_blocks.1.0.temopral_conv.conv4.3.weight\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 output_blocks.10.0.emb_layers.1.weight\r\n",
      "-rw-r--r-- 1 root root 7.1M Feb  4 16:03 output_blocks.10.0.in_layers.2.weight\r\n",
      "-rw-r--r-- 1 root root 3.6M Feb  4 16:03 output_blocks.10.0.out_layers.3.weight\r\n",
      "-rw-r--r-- 1 root root 800K Feb  4 16:03 output_blocks.10.0.skip_connection.weight\r\n",
      "-rw-r--r-- 1 root root 1.2M Feb  4 16:03 output_blocks.10.0.temopral_conv.conv1.2.weight\r\n",
      "-rw-r--r-- 1 root root 1.2M Feb  4 16:03 output_blocks.10.0.temopral_conv.conv2.3.weight\r\n",
      "-rw-r--r-- 1 root root 1.2M Feb  4 16:03 output_blocks.10.0.temopral_conv.conv3.3.weight\r\n",
      "-rw-r--r-- 1 root root 1.2M Feb  4 16:03 output_blocks.10.0.temopral_conv.conv4.3.weight\r\n",
      "-rw-r--r-- 1 root root  10K Feb  4 16:03 output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 output_blocks.10.2.proj_in.weight\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 output_blocks.10.2.proj_out.weight\r\n",
      "-rw-r--r-- 1 root root  10K Feb  4 16:03 output_blocks.10.2.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 output_blocks.11.0.emb_layers.1.weight\r\n",
      "-rw-r--r-- 1 root root 7.1M Feb  4 16:03 output_blocks.11.0.in_layers.2.weight\r\n",
      "-rw-r--r-- 1 root root 3.6M Feb  4 16:03 output_blocks.11.0.out_layers.3.weight\r\n",
      "-rw-r--r-- 1 root root 800K Feb  4 16:03 output_blocks.11.0.skip_connection.weight\r\n",
      "-rw-r--r-- 1 root root 1.2M Feb  4 16:03 output_blocks.11.0.temopral_conv.conv1.2.weight\r\n",
      "-rw-r--r-- 1 root root 1.2M Feb  4 16:03 output_blocks.11.0.temopral_conv.conv2.3.weight\r\n",
      "-rw-r--r-- 1 root root 1.2M Feb  4 16:03 output_blocks.11.0.temopral_conv.conv3.3.weight\r\n",
      "-rw-r--r-- 1 root root 1.2M Feb  4 16:03 output_blocks.11.0.temopral_conv.conv4.3.weight\r\n",
      "-rw-r--r-- 1 root root  10K Feb  4 16:03 output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 output_blocks.11.2.proj_in.weight\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 output_blocks.11.2.proj_out.weight\r\n",
      "-rw-r--r-- 1 root root  10K Feb  4 16:03 output_blocks.11.2.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.2.0.emb_layers.1.bias\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 output_blocks.2.0.emb_layers.1.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.2.0.in_layers.2.bias\r\n",
      "-rw-r--r-- 1 root root 113M Feb  4 16:03 output_blocks.2.0.in_layers.2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.2.0.out_layers.3.bias\r\n",
      "-rw-r--r-- 1 root root  57M Feb  4 16:03 output_blocks.2.0.out_layers.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.2.0.skip_connection.bias\r\n",
      "-rw-r--r-- 1 root root  13M Feb  4 16:03 output_blocks.2.0.skip_connection.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.2.0.temopral_conv.conv1.2.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 output_blocks.2.0.temopral_conv.conv1.2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.2.0.temopral_conv.conv2.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 output_blocks.2.0.temopral_conv.conv2.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.2.0.temopral_conv.conv3.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 output_blocks.2.0.temopral_conv.conv3.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.2.0.temopral_conv.conv4.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 output_blocks.2.0.temopral_conv.conv4.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.2.1.conv.bias\r\n",
      "-rw-r--r-- 1 root root  57M Feb  4 16:03 output_blocks.2.1.conv.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.0.emb_layers.1.bias\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 output_blocks.3.0.emb_layers.1.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.0.in_layers.2.bias\r\n",
      "-rw-r--r-- 1 root root 113M Feb  4 16:03 output_blocks.3.0.in_layers.2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.0.out_layers.3.bias\r\n",
      "-rw-r--r-- 1 root root  57M Feb  4 16:03 output_blocks.3.0.out_layers.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.0.skip_connection.bias\r\n",
      "-rw-r--r-- 1 root root  13M Feb  4 16:03 output_blocks.3.0.skip_connection.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.0.temopral_conv.conv1.2.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 output_blocks.3.0.temopral_conv.conv1.2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.0.temopral_conv.conv2.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 output_blocks.3.0.temopral_conv.conv2.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.0.temopral_conv.conv3.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 output_blocks.3.0.temopral_conv.conv3.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.0.temopral_conv.conv4.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 output_blocks.3.0.temopral_conv.conv4.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.1.proj_in.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.1.proj_out.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias\r\n",
      "-rw-r--r-- 1 root root  40K Feb  4 16:03 output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.1.transformer_blocks.0.ff.net.2.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.1.transformer_blocks.0.norm1.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.1.transformer_blocks.0.norm1.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.1.transformer_blocks.0.norm2.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.1.transformer_blocks.0.norm2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.1.transformer_blocks.0.norm3.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.1.transformer_blocks.0.norm3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.2.proj_in.bias\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 output_blocks.3.2.proj_in.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.2.proj_out.bias\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 output_blocks.3.2.proj_out.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.2.transformer_blocks.0.attn1.to_out.0.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.2.transformer_blocks.0.attn2.to_out.0.bias\r\n",
      "-rw-r--r-- 1 root root  40K Feb  4 16:03 output_blocks.3.2.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.2.transformer_blocks.0.ff.net.2.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.2.transformer_blocks.0.norm1.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.2.transformer_blocks.0.norm1.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.2.transformer_blocks.0.norm2.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.2.transformer_blocks.0.norm2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.2.transformer_blocks.0.norm3.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.3.2.transformer_blocks.0.norm3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.0.emb_layers.1.bias\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 output_blocks.4.0.emb_layers.1.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.0.in_layers.2.bias\r\n",
      "-rw-r--r-- 1 root root 113M Feb  4 16:03 output_blocks.4.0.in_layers.2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.0.out_layers.3.bias\r\n",
      "-rw-r--r-- 1 root root  57M Feb  4 16:03 output_blocks.4.0.out_layers.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.0.skip_connection.bias\r\n",
      "-rw-r--r-- 1 root root  13M Feb  4 16:03 output_blocks.4.0.skip_connection.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.0.temopral_conv.conv1.2.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 output_blocks.4.0.temopral_conv.conv1.2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.0.temopral_conv.conv2.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 output_blocks.4.0.temopral_conv.conv2.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.0.temopral_conv.conv3.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 output_blocks.4.0.temopral_conv.conv3.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.0.temopral_conv.conv4.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 output_blocks.4.0.temopral_conv.conv4.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.1.proj_in.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.1.proj_out.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias\r\n",
      "-rw-r--r-- 1 root root  40K Feb  4 16:03 output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.1.transformer_blocks.0.ff.net.2.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.1.transformer_blocks.0.norm1.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.1.transformer_blocks.0.norm1.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.1.transformer_blocks.0.norm2.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.1.transformer_blocks.0.norm2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.1.transformer_blocks.0.norm3.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.1.transformer_blocks.0.norm3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.2.proj_in.bias\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 output_blocks.4.2.proj_in.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.2.proj_out.bias\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 output_blocks.4.2.proj_out.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.2.transformer_blocks.0.attn1.to_out.0.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.2.transformer_blocks.0.attn2.to_out.0.bias\r\n",
      "-rw-r--r-- 1 root root  40K Feb  4 16:03 output_blocks.4.2.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.2.transformer_blocks.0.ff.net.2.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.2.transformer_blocks.0.norm1.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.2.transformer_blocks.0.norm1.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.2.transformer_blocks.0.norm2.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.2.transformer_blocks.0.norm2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.2.transformer_blocks.0.norm3.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.4.2.transformer_blocks.0.norm3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.0.emb_layers.1.bias\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 output_blocks.5.0.emb_layers.1.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.0.in_layers.2.bias\r\n",
      "-rw-r--r-- 1 root root  85M Feb  4 16:03 output_blocks.5.0.in_layers.2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.0.out_layers.3.bias\r\n",
      "-rw-r--r-- 1 root root  57M Feb  4 16:03 output_blocks.5.0.out_layers.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.0.skip_connection.bias\r\n",
      "-rw-r--r-- 1 root root 9.4M Feb  4 16:03 output_blocks.5.0.skip_connection.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.0.temopral_conv.conv1.2.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 output_blocks.5.0.temopral_conv.conv1.2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.0.temopral_conv.conv2.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 output_blocks.5.0.temopral_conv.conv2.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.0.temopral_conv.conv3.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 output_blocks.5.0.temopral_conv.conv3.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.0.temopral_conv.conv4.3.bias\r\n",
      "-rw-r--r-- 1 root root  19M Feb  4 16:03 output_blocks.5.0.temopral_conv.conv4.3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.1.proj_in.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.1.proj_out.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias\r\n",
      "-rw-r--r-- 1 root root  40K Feb  4 16:03 output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.1.transformer_blocks.0.ff.net.2.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.1.transformer_blocks.0.norm1.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.1.transformer_blocks.0.norm1.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.1.transformer_blocks.0.norm2.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.1.transformer_blocks.0.norm2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.1.transformer_blocks.0.norm3.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.1.transformer_blocks.0.norm3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.2.proj_in.bias\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 output_blocks.5.2.proj_in.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.2.proj_out.bias\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 output_blocks.5.2.proj_out.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.2.transformer_blocks.0.attn1.to_out.0.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.2.transformer_blocks.0.attn2.to_out.0.bias\r\n",
      "-rw-r--r-- 1 root root  40K Feb  4 16:03 output_blocks.5.2.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.2.transformer_blocks.0.ff.net.2.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.2.transformer_blocks.0.norm1.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.2.transformer_blocks.0.norm1.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.2.transformer_blocks.0.norm2.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.2.transformer_blocks.0.norm2.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.2.transformer_blocks.0.norm3.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.2.transformer_blocks.0.norm3.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 output_blocks.5.3.conv.bias\r\n",
      "-rw-r--r-- 1 root root  57M Feb  4 16:03 output_blocks.5.3.conv.weight\r\n",
      "-rw-r--r-- 1 root root 3.2M Feb  4 16:03 output_blocks.6.0.emb_layers.1.weight\r\n",
      "-rw-r--r-- 1 root root  43M Feb  4 16:03 output_blocks.6.0.in_layers.2.weight\r\n",
      "-rw-r--r-- 1 root root  15M Feb  4 16:03 output_blocks.6.0.out_layers.3.weight\r\n",
      "-rw-r--r-- 1 root root 4.7M Feb  4 16:03 output_blocks.6.0.skip_connection.weight\r\n",
      "-rw-r--r-- 1 root root 4.7M Feb  4 16:03 output_blocks.6.0.temopral_conv.conv1.2.weight\r\n",
      "-rw-r--r-- 1 root root 4.7M Feb  4 16:03 output_blocks.6.0.temopral_conv.conv2.3.weight\r\n",
      "-rw-r--r-- 1 root root 4.7M Feb  4 16:03 output_blocks.6.0.temopral_conv.conv3.3.weight\r\n",
      "-rw-r--r-- 1 root root 4.7M Feb  4 16:03 output_blocks.6.0.temopral_conv.conv4.3.weight\r\n",
      "-rw-r--r-- 1 root root  20K Feb  4 16:03 output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 output_blocks.6.2.proj_in.weight\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 output_blocks.6.2.proj_out.weight\r\n",
      "-rw-r--r-- 1 root root  20K Feb  4 16:03 output_blocks.6.2.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 3.2M Feb  4 16:03 output_blocks.7.0.emb_layers.1.weight\r\n",
      "-rw-r--r-- 1 root root  29M Feb  4 16:03 output_blocks.7.0.in_layers.2.weight\r\n",
      "-rw-r--r-- 1 root root  15M Feb  4 16:03 output_blocks.7.0.out_layers.3.weight\r\n",
      "-rw-r--r-- 1 root root 3.2M Feb  4 16:03 output_blocks.7.0.skip_connection.weight\r\n",
      "-rw-r--r-- 1 root root 4.7M Feb  4 16:03 output_blocks.7.0.temopral_conv.conv1.2.weight\r\n",
      "-rw-r--r-- 1 root root 4.7M Feb  4 16:03 output_blocks.7.0.temopral_conv.conv2.3.weight\r\n",
      "-rw-r--r-- 1 root root 4.7M Feb  4 16:03 output_blocks.7.0.temopral_conv.conv3.3.weight\r\n",
      "-rw-r--r-- 1 root root 4.7M Feb  4 16:03 output_blocks.7.0.temopral_conv.conv4.3.weight\r\n",
      "-rw-r--r-- 1 root root  20K Feb  4 16:03 output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 output_blocks.7.2.proj_in.weight\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 output_blocks.7.2.proj_out.weight\r\n",
      "-rw-r--r-- 1 root root  20K Feb  4 16:03 output_blocks.7.2.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 3.2M Feb  4 16:03 output_blocks.8.0.emb_layers.1.weight\r\n",
      "-rw-r--r-- 1 root root  22M Feb  4 16:03 output_blocks.8.0.in_layers.2.weight\r\n",
      "-rw-r--r-- 1 root root  15M Feb  4 16:03 output_blocks.8.0.out_layers.3.weight\r\n",
      "-rw-r--r-- 1 root root 2.4M Feb  4 16:03 output_blocks.8.0.skip_connection.weight\r\n",
      "-rw-r--r-- 1 root root 4.7M Feb  4 16:03 output_blocks.8.0.temopral_conv.conv1.2.weight\r\n",
      "-rw-r--r-- 1 root root 4.7M Feb  4 16:03 output_blocks.8.0.temopral_conv.conv2.3.weight\r\n",
      "-rw-r--r-- 1 root root 4.7M Feb  4 16:03 output_blocks.8.0.temopral_conv.conv3.3.weight\r\n",
      "-rw-r--r-- 1 root root 4.7M Feb  4 16:03 output_blocks.8.0.temopral_conv.conv4.3.weight\r\n",
      "-rw-r--r-- 1 root root  20K Feb  4 16:03 output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 output_blocks.8.2.proj_in.weight\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 output_blocks.8.2.proj_out.weight\r\n",
      "-rw-r--r-- 1 root root  20K Feb  4 16:03 output_blocks.8.2.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root  15M Feb  4 16:03 output_blocks.8.3.conv.weight\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 output_blocks.9.0.emb_layers.1.weight\r\n",
      "-rw-r--r-- 1 root root  11M Feb  4 16:03 output_blocks.9.0.in_layers.2.weight\r\n",
      "-rw-r--r-- 1 root root 3.6M Feb  4 16:03 output_blocks.9.0.out_layers.3.weight\r\n",
      "-rw-r--r-- 1 root root 1.2M Feb  4 16:03 output_blocks.9.0.skip_connection.weight\r\n",
      "-rw-r--r-- 1 root root 1.2M Feb  4 16:03 output_blocks.9.0.temopral_conv.conv1.2.weight\r\n",
      "-rw-r--r-- 1 root root 1.2M Feb  4 16:03 output_blocks.9.0.temopral_conv.conv2.3.weight\r\n",
      "-rw-r--r-- 1 root root 1.2M Feb  4 16:03 output_blocks.9.0.temopral_conv.conv3.3.weight\r\n",
      "-rw-r--r-- 1 root root 1.2M Feb  4 16:03 output_blocks.9.0.temopral_conv.conv4.3.weight\r\n",
      "-rw-r--r-- 1 root root  10K Feb  4 16:03 output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 output_blocks.9.2.proj_in.weight\r\n",
      "-rw-r--r-- 1 root root 400K Feb  4 16:03 output_blocks.9.2.proj_out.weight\r\n",
      "-rw-r--r-- 1 root root  10K Feb  4 16:03 output_blocks.9.2.transformer_blocks.0.ff.net.0.proj.bias\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 time_embed.0.bias\r\n",
      "-rw-r--r-- 1 root root 1.6M Feb  4 16:03 time_embed.0.weight\r\n",
      "-rw-r--r-- 1 root root 5.0K Feb  4 16:03 time_embed.2.bias\r\n",
      "-rw-r--r-- 1 root root 6.3M Feb  4 16:03 time_embed.2.weight\r\n"
     ]
    }
   ],
   "source": [
    "# check onnx model\n",
    "!ls -hl ./models/onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c88d3f9",
   "metadata": {},
   "source": [
    "### Convert models from pytorch to Onnx\n",
    "\n",
    "Convert Models to Onnx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3ab8bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/tensorrt/bin/trtexec\r\n"
     ]
    }
   ],
   "source": [
    "# we have trtexec pre-installed in the docker container\n",
    "!which trtexec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed574079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8601] # trtexec --onnx=./models/onnx/model.onnx --saveEngine=./models/trt_fp16.engine --fp16 --skipInference\n",
      "[02/04/2024-16:20:29] [I] === Model Options ===\n",
      "[02/04/2024-16:20:29] [I] Format: ONNX\n",
      "[02/04/2024-16:20:29] [I] Model: ./models/onnx/model.onnx\n",
      "[02/04/2024-16:20:29] [I] Output:\n",
      "[02/04/2024-16:20:29] [I] === Build Options ===\n",
      "[02/04/2024-16:20:29] [I] Max batch: explicit batch\n",
      "[02/04/2024-16:20:29] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default\n",
      "[02/04/2024-16:20:29] [I] minTiming: 1\n",
      "[02/04/2024-16:20:29] [I] avgTiming: 8\n",
      "[02/04/2024-16:20:29] [I] Precision: FP32+FP16\n",
      "[02/04/2024-16:20:29] [I] LayerPrecisions: \n",
      "[02/04/2024-16:20:29] [I] Layer Device Types: \n",
      "[02/04/2024-16:20:29] [I] Calibration: \n",
      "[02/04/2024-16:20:29] [I] Refit: Disabled\n",
      "[02/04/2024-16:20:29] [I] Version Compatible: Disabled\n",
      "[02/04/2024-16:20:29] [I] TensorRT runtime: full\n",
      "[02/04/2024-16:20:29] [I] Lean DLL Path: \n",
      "[02/04/2024-16:20:29] [I] Tempfile Controls: { in_memory: allow, temporary: allow }\n",
      "[02/04/2024-16:20:29] [I] Exclude Lean Runtime: Disabled\n",
      "[02/04/2024-16:20:29] [I] Sparsity: Disabled\n",
      "[02/04/2024-16:20:29] [I] Safe mode: Disabled\n",
      "[02/04/2024-16:20:29] [I] Build DLA standalone loadable: Disabled\n",
      "[02/04/2024-16:20:29] [I] Allow GPU fallback for DLA: Disabled\n",
      "[02/04/2024-16:20:29] [I] DirectIO mode: Disabled\n",
      "[02/04/2024-16:20:29] [I] Restricted mode: Disabled\n",
      "[02/04/2024-16:20:29] [I] Skip inference: Enabled\n",
      "[02/04/2024-16:20:29] [I] Save engine: ./models/trt_fp16.engine\n",
      "[02/04/2024-16:20:29] [I] Load engine: \n",
      "[02/04/2024-16:20:29] [I] Profiling verbosity: 0\n",
      "[02/04/2024-16:20:29] [I] Tactic sources: Using default tactic sources\n",
      "[02/04/2024-16:20:29] [I] timingCacheMode: local\n",
      "[02/04/2024-16:20:29] [I] timingCacheFile: \n",
      "[02/04/2024-16:20:29] [I] Heuristic: Disabled\n",
      "[02/04/2024-16:20:29] [I] Preview Features: Use default preview flags.\n",
      "[02/04/2024-16:20:29] [I] MaxAuxStreams: -1\n",
      "[02/04/2024-16:20:29] [I] BuilderOptimizationLevel: -1\n",
      "[02/04/2024-16:20:29] [I] Input(s)s format: fp32:CHW\n",
      "[02/04/2024-16:20:29] [I] Output(s)s format: fp32:CHW\n",
      "[02/04/2024-16:20:29] [I] Input build shapes: model\n",
      "[02/04/2024-16:20:29] [I] Input calibration shapes: model\n",
      "[02/04/2024-16:20:29] [I] === System Options ===\n",
      "[02/04/2024-16:20:29] [I] Device: 0\n",
      "[02/04/2024-16:20:29] [I] DLACore: \n",
      "[02/04/2024-16:20:29] [I] Plugins:\n",
      "[02/04/2024-16:20:29] [I] setPluginsToSerialize:\n",
      "[02/04/2024-16:20:29] [I] dynamicPlugins:\n",
      "[02/04/2024-16:20:29] [I] ignoreParsedPluginLibs: 0\n",
      "[02/04/2024-16:20:29] [I] \n",
      "[02/04/2024-16:20:29] [I] === Inference Options ===\n",
      "[02/04/2024-16:20:29] [I] Batch: Explicit\n",
      "[02/04/2024-16:20:29] [I] Input inference shapes: model\n",
      "[02/04/2024-16:20:29] [I] Iterations: 10\n",
      "[02/04/2024-16:20:29] [I] Duration: 3s (+ 200ms warm up)\n",
      "[02/04/2024-16:20:29] [I] Sleep time: 0ms\n",
      "[02/04/2024-16:20:29] [I] Idle time: 0ms\n",
      "[02/04/2024-16:20:29] [I] Inference Streams: 1\n",
      "[02/04/2024-16:20:29] [I] ExposeDMA: Disabled\n",
      "[02/04/2024-16:20:29] [I] Data transfers: Enabled\n",
      "[02/04/2024-16:20:29] [I] Spin-wait: Disabled\n",
      "[02/04/2024-16:20:29] [I] Multithreading: Disabled\n",
      "[02/04/2024-16:20:29] [I] CUDA Graph: Disabled\n",
      "[02/04/2024-16:20:29] [I] Separate profiling: Disabled\n",
      "[02/04/2024-16:20:29] [I] Time Deserialize: Disabled\n",
      "[02/04/2024-16:20:29] [I] Time Refit: Disabled\n",
      "[02/04/2024-16:20:29] [I] NVTX verbosity: 0\n",
      "[02/04/2024-16:20:29] [I] Persistent Cache Ratio: 0\n",
      "[02/04/2024-16:20:29] [I] Inputs:\n",
      "[02/04/2024-16:20:29] [I] === Reporting Options ===\n",
      "[02/04/2024-16:20:29] [I] Verbose: Disabled\n",
      "[02/04/2024-16:20:29] [I] Averages: 10 inferences\n",
      "[02/04/2024-16:20:29] [I] Percentiles: 90,95,99\n",
      "[02/04/2024-16:20:29] [I] Dump refittable layers:Disabled\n",
      "[02/04/2024-16:20:29] [I] Dump output: Disabled\n",
      "[02/04/2024-16:20:29] [I] Profile: Disabled\n",
      "[02/04/2024-16:20:29] [I] Export timing to JSON file: \n",
      "[02/04/2024-16:20:29] [I] Export output to JSON file: \n",
      "[02/04/2024-16:20:29] [I] Export profile to JSON file: \n",
      "[02/04/2024-16:20:29] [I] \n",
      "[02/04/2024-16:20:29] [I] === Device Information ===\n",
      "[02/04/2024-16:20:29] [I] Selected Device: NVIDIA RTX A6000\n",
      "[02/04/2024-16:20:29] [I] Compute Capability: 8.6\n",
      "[02/04/2024-16:20:29] [I] SMs: 84\n",
      "[02/04/2024-16:20:29] [I] Device Global Memory: 48676 MiB\n",
      "[02/04/2024-16:20:29] [I] Shared Memory per SM: 100 KiB\n",
      "[02/04/2024-16:20:29] [I] Memory Bus Width: 384 bits (ECC disabled)\n",
      "[02/04/2024-16:20:29] [I] Application Compute Clock Rate: 1.8 GHz\n",
      "[02/04/2024-16:20:29] [I] Application Memory Clock Rate: 8.001 GHz\n",
      "[02/04/2024-16:20:29] [I] \n",
      "[02/04/2024-16:20:29] [I] Note: The application clock rates do not reflect the actual clock rates that the GPU is currently running at.\n",
      "[02/04/2024-16:20:29] [I] \n",
      "[02/04/2024-16:20:29] [I] TensorRT version: 8.6.1\n",
      "[02/04/2024-16:20:29] [I] Loading standard plugins\n",
      "[02/04/2024-16:20:30] [I] [TRT] [MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 19, GPU 270 (MiB)\n",
      "[02/04/2024-16:20:34] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +1444, GPU +268, now: CPU 1540, GPU 538 (MiB)\n",
      "[02/04/2024-16:20:34] [I] Start parsing network model.\n",
      "[02/04/2024-16:20:34] [I] [TRT] ----------------------------------------------------------------\n",
      "[02/04/2024-16:20:34] [I] [TRT] Input filename:   ./models/onnx/model.onnx\n",
      "[02/04/2024-16:20:34] [I] [TRT] ONNX IR version:  0.0.8\n",
      "[02/04/2024-16:20:34] [I] [TRT] Opset version:    17\n",
      "[02/04/2024-16:20:34] [I] [TRT] Producer name:    pytorch\n",
      "[02/04/2024-16:20:34] [I] [TRT] Producer version: 2.1.0\n",
      "[02/04/2024-16:20:34] [I] [TRT] Domain:           \n",
      "[02/04/2024-16:20:34] [I] [TRT] Model version:    0\n",
      "[02/04/2024-16:20:34] [I] [TRT] Doc string:       \n",
      "[02/04/2024-16:20:34] [I] [TRT] ----------------------------------------------------------------\n",
      "[02/04/2024-16:20:49] [W] [TRT] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[02/04/2024-16:20:49] [I] Finished parsing network model. Parse time: 14.7628\n",
      "[02/04/2024-16:20:51] [I] [TRT] Graph optimization time: 1.39304 seconds.\n",
      "[02/04/2024-16:20:51] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 7211, GPU 4080 (MiB)\n",
      "[02/04/2024-16:20:51] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 7211, GPU 4090 (MiB)\n",
      "[02/04/2024-16:20:51] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[02/04/2024-16:30:59] [I] [TRT] Detected 3 inputs and 1 output network tensors.\n",
      "[02/04/2024-16:31:11] [I] [TRT] Total Host Persistent Memory: 1165920\n",
      "[02/04/2024-16:31:11] [I] [TRT] Total Device Persistent Memory: 114688\n",
      "[02/04/2024-16:31:11] [I] [TRT] Total Scratch Memory: 184614912\n",
      "[02/04/2024-16:31:11] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 1789 MiB, GPU 1687 MiB\n",
      "[02/04/2024-16:31:11] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 2016 steps to complete.\n",
      "[02/04/2024-16:31:11] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 612.328ms to assign 25 blocks to 2016 nodes requiring 320168960 bytes.\n",
      "[02/04/2024-16:31:11] [I] [TRT] Total Activation Memory: 320165888\n",
      "[02/04/2024-16:31:13] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 11795, GPU 9060 (MiB)\n",
      "[02/04/2024-16:31:13] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +1, GPU +8, now: CPU 11796, GPU 9068 (MiB)\n",
      "[02/04/2024-16:31:13] [W] [TRT] TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
      "[02/04/2024-16:31:13] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
      "[02/04/2024-16:31:13] [W] [TRT] Check verbose logs for the list of affected weights.\n",
      "[02/04/2024-16:31:13] [W] [TRT] - 875 weights are affected by this issue: Detected subnormal FP16 values.\n",
      "[02/04/2024-16:31:13] [W] [TRT] - 368 weights are affected by this issue: Detected values less than smallest positive FP16 subnormal value and converted them to the FP16 minimum subnormalized value.\n",
      "[02/04/2024-16:31:13] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +1686, GPU +2692, now: CPU 1686, GPU 2692 (MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/04/2024-16:31:16] [I] Engine built in 646.805 sec.\n",
      "[02/04/2024-16:31:16] [I] [TRT] Loaded engine size: 2716 MiB\n",
      "[02/04/2024-16:31:17] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 11672, GPU 7678 (MiB)\n",
      "[02/04/2024-16:31:17] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 11672, GPU 7686 (MiB)\n",
      "[02/04/2024-16:31:17] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +2691, now: CPU 0, GPU 2691 (MiB)\n",
      "[02/04/2024-16:31:17] [I] Engine deserialized in 1.06871 sec.\n",
      "[02/04/2024-16:31:17] [I] Skipped inference phase since --skipInference is added.\n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8601] # trtexec --onnx=./models/onnx/model.onnx --saveEngine=./models/trt_fp16.engine --fp16 --skipInference\n"
     ]
    }
   ],
   "source": [
    "!trtexec --onnx=./models/onnx/model.onnx \\\n",
    "  --saveEngine=./models/trt_fp16.engine \\\n",
    "  --fp16 --skipInference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96a0e1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 2.7G Feb  4 16:31 ./models/trt_fp16.engine\r\n"
     ]
    }
   ],
   "source": [
    "# check tensorrt engine file\n",
    "!ls -lh ./models/trt_fp16.engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7304ece3",
   "metadata": {},
   "source": [
    "## Compare Inference Speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79be471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0\n",
    "torch_model_dir = \"./models\"\n",
    "trt_model_dir = \"./models/trt_fp16.engine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acdeeae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import tensorrt as trt\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import json\n",
    "from os import path as osp\n",
    "from unet_sd import UNetSD\n",
    "from cupyx.profiler import benchmark\n",
    "\n",
    "trt.init_libnvinfer_plugins(None, \"\")\n",
    "\n",
    "config = json.load(open(osp.join(torch_model_dir, \"configuration.json\")))\n",
    "\n",
    "cfg = config[\"model\"][\"model_cfg\"]\n",
    "cfg['temporal_attention'] = True if cfg['temporal_attention'] == 'True' else False\n",
    "\n",
    "# Initialize unet\n",
    "sd_model = UNetSD(\n",
    "    in_dim=cfg['unet_in_dim'],\n",
    "    dim=cfg['unet_dim'],\n",
    "    y_dim=cfg['unet_y_dim'],\n",
    "    context_dim=cfg['unet_context_dim'],\n",
    "    out_dim=cfg['unet_out_dim'],\n",
    "    dim_mult=cfg['unet_dim_mult'],\n",
    "    num_heads=cfg['unet_num_heads'],\n",
    "    head_dim=cfg['unet_head_dim'],\n",
    "    num_res_blocks=cfg['unet_res_blocks'],\n",
    "    attn_scales=cfg['unet_attn_scales'],\n",
    "    dropout=cfg['unet_dropout'],\n",
    "    temporal_attention=cfg['temporal_attention'])\n",
    "sd_model.load_state_dict(\n",
    "    torch.load(\n",
    "        osp.join(torch_model_dir, config[\"model\"][\"model_args\"][\"ckpt_unet\"])),\n",
    "    strict=True)\n",
    "sd_model.eval()\n",
    "# sd_model.half()\n",
    "sd_model.to(device)\n",
    "\n",
    "\n",
    "from EngineUtil import Engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e15d746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TensorRT engine: ./models/trt_fp16.engine\n",
      "[W] 'colored' module is not installed, will not use colors when logging. To enable colors, please install the 'colored' module: python3 -m pip install colored\n",
      "[I] Loading bytes from ./models/trt_fp16.engine\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_warmup = 10\n",
    "n_repeat = 10\n",
    "\n",
    "# create dummy inputs\n",
    "dummy_x = torch.randn(1, 4, config[\"model\"][\"model_args\"][\"max_frames\"],\n",
    "     config[\"model\"][\"model_args\"].get(\"width\", 256)//8,\n",
    "     config[\"model\"][\"model_args\"].get(\"width\", 256)//8,\n",
    "     dtype=torch.float32, device=0).cuda()   # noise tensor\n",
    "dummy_t = torch.randint(0, 1000, (1,), dtype=torch.int32, device=0).cuda()  # time steps tensor\n",
    "dummy_y = torch.randn(1, 77, 1024, device=0).cuda()  # text embeddings tensor\n",
    "\n",
    "\n",
    "def pytorch_model():\n",
    "    return sd_model(dummy_x, dummy_t, dummy_y)\n",
    "\n",
    "engine = Engine(trt_model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962d4a7a",
   "metadata": {},
   "source": [
    "##### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4427ff73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch_model       :    CPU: 119984.063 us   +/- 190.493 (min: 119600.165 / max: 120252.957) us     GPU-0: 211981.627 us   +/- 497.686 (min: 211470.337 / max: 212790.268) us\n",
      "infer               :    CPU: 57663.214 us   +/- 198.205 (min: 57274.309 / max: 57940.356) us     GPU-0: 90644.787 us   +/- 235.943 (min: 90256.386 / max: 91005.951) us\n"
     ]
    }
   ],
   "source": [
    "pytorch_benchmark = benchmark(pytorch_model, (), n_warmup=n_warmup, n_repeat=n_repeat)\n",
    "\n",
    "trt_benchmark = benchmark(engine.infer,\n",
    "    ({\"x\":dummy_x, \"t\":dummy_t, \"y\":dummy_y}, ),\n",
    "    n_warmup=n_warmup, n_repeat=n_repeat\n",
    ")\n",
    "\n",
    "# see https://docs.cupy.dev/en/stable/reference/generated/cupyx.profiler._time._PerfCaseResult.html#cupyx.profiler._time._PerfCaseResult\n",
    "print(pytorch_benchmark)\n",
    "print(trt_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86b749d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch inference 332 ms, trt inference used 149 ms; Speedup: 2.228187919463087X\n"
     ]
    }
   ],
   "source": [
    "pytorch_time = 120 + 212\n",
    "trt_time = 58 + 91\n",
    "speedup = pytorch_time/trt_time\n",
    "print(f\"pytorch inference {pytorch_time} ms, trt inference used {trt_time} ms; Speedup: {speedup}X\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
